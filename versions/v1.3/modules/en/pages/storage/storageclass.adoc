= StorageClass

A StorageClass allows administrators to describe the *classes* of storage they offer. Different {longhorn-product-name} StorageClasses might map to replica policies, or to node schedule policies, or disk schedule policies determined by the cluster administrators. This concept is sometimes called *profiles* in other storage systems.

[NOTE]
====
For support with other storage, please refer to xref:./csidriver.adoc[Third-Party Storage Support]
====

== Creating a StorageClass

You can create one or more StorageClasses from the **Advanced > StorageClasses** page.

image::storageclass/create_storageclasses_entry.png[]

[NOTE]
====
After a StorageClass is created, nothing can be changed except `Description`.
====

=== Header Section

. *Name*: name of the StorageClass
. *Description* (optional): description of the StorageClass

image::storageclass/create_storageclasses_header_sections.png[]

=== Parameters Tab

==== Number of Replicas

The number of replicas created for each volume in {longhorn-product-name}. Defaults to `3`.

image::storageclass/create_storageclasses_replicas.png[]

==== Stale Replica Timeout

Determines when {longhorn-product-name} would clean up an error replica after the replica's status is ERROR. The unit is minute. Defaults to `30` minutes in Harvester.

image::storageclass/create_storageclasses_stale_timeout.png[]

==== Node Selector (Optional)

Select the node tags to be matched in the volume scheduling stage. You can add node tags by going to **Host > Edit Config**.

image::storageclass/create_storageclasses_node_selector.png[]

==== Disk Selector (Optional)

Select the disk tags to be matched in the volume scheduling stage. You can add disk tags by going to **Host > Edit Config**.

image::storageclass/create_storageclasses_disk_selector.png[]

==== Migratable

Whether xref:../virtual-machines/live-migration.adoc[Live Migration] is supported. Defaults to `Yes`.

image::storageclass/create_storageclasses_migratable.png[]

**Allow Volume Expansion**


Volumes dynamically created by a StorageClass will have the reclaim policy specified in the `reclaimPolicy` field of the class. The `Delete` mode is used by default.

. `Delete`: Deletes volumes and the underlying devices when the volume claim is deleted.
. `Retain`: Retains the volume for manual cleanup.

image::storageclass/customize_tab_reclaim_policy.png[]

**Allow Volume Expansion**

Volumes can be configured to be expandable. This feature is `Enabled` by default, which allows users to resize the volume by editing the corresponding PVC object.

image::storageclass/customize_tab_allow_vol_expansion.png[]

[NOTE]
====
You can only use the volume expansion feature to grow a Volume, not to shrink it.
====

==== Volume Binding Mode

The `volumeBindingMode` field controls when volume binding and dynamic provisioning should occur. The `Immediate` mode is used by default.

. `Immediate`: Binds and provisions a persistent volume once the PersistentVolumeClaim is created.
. `WaitForFirstConsumer`: Binds and provisions a persistent volume once a VM using the PersistentVolumeClaim is created.

image::storageclass/customize_tab_vol_binding_mode.png[]

== Data Locality Settings

You can use the `dataLocality` parameter when at least one replica of a {longhorn-product-name} volume must be scheduled on the same node as the pod that uses the volume (whenever possible).

{harvester-product-name} officially supports data locality. This applies even to volumes created from xref:../virtual-machines/vm-images/upload-image.adoc[images]. To configure data locality, create a new StorageClass on the {harvester-product-name} UI (*Storage Classess -> Create -> Parameters*) and then add the following parameter:

* *Key*: `dataLocality`
* *Value*: `disabled` or `best-effort`

image::storageclass/data-locality.png[]

=== Data Locality Options

{harvester-product-name} currently supports the following options:

* `disabled`: When applied, {longhorn-product-name} may or may not schedule a replica on the same node as the pod that uses the volume. This is the default option.
* `best-effort`: When applied, {longhorn-product-name} always attempts to schedule a replica on the same node as the pod that uses the volume. {longhorn-product-name} does not stop the volume even when a local replica is unavailable because of an environmental limitation (for example, insufficient disk space or incompatible disk tags).

[NOTE]
====
{longhorn-product-name} provides a third option called `strict-local`, which forces {longhorn-product-name} to keep only one replica on the same node as the pod that uses the volume. {harvester-product-name} does not support this option because it can affect certain operations such as xref:../virtual-machines/live-migration.adoc[VM Live Migration].
====

For more information, see https://documentation.suse.com/cloudnative/storage/1.7.0/en/high-availability/data-locality.html[Data Locality] in the {longhorn-product-name} documentation.

== Appendix - Use Case

=== HDD Scenario

With the introduction of _StorageClass_, users can now use *HDDs* for tiered or archived cold storage.

[CAUTION]
====
HDD is not recommended for guest RKE2 clusters or VMs with good performance disk requirements.
====

==== Recommended Practice

First, add your HDD on the `Host` page and specify the disk tags as needed, such as `HDD` or `ColdStorage`. For more information on how to add extra disks and disk tags, see xref:../hosts/hosts.adoc#_multi_disk_management[Multi-disk Management] for details.

image::storageclass/add_hdd_on_host_page.png[]

image::storageclass/add_tags.png[]

Then, create a new `StorageClass` for the HDD (use the above disk tags). For hard drives with large capacity but slow performance, the number of replicas can be reduced to improve performance.

image::storageclass/create_hdd_storageclass.png[]

You can now create a volume using the above `StorageClass` with HDDs mostly for cold storage or archiving purpose.

image::storageclass/create_volume_hdd.png[]
